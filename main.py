# main.py - Application entry point
import sys
import threading
from PyQt6.QtWidgets import QApplication
from src.gui.main_window import MainWindow
from apscheduler.schedulers.background import BackgroundScheduler
from src.core.config_manager import load_rss_configs, save_config, load_affiliate_config
from src.core.rss_fetcher import parse_feed, generate_entry_id, fetch_webpage_content
from src.core.qq_pusher import send_group_message
from src.core.affiliate_converter import AffiliateConverter
from src.utils.text_cleaner import summarize_text, clean_html_tags
import logging
import os
import json
from logging.handlers import RotatingFileHandler

# åˆ›å»ºå…¨å±€åœæ­¢æ ‡å¿—
_stop_flag = threading.Event()

# åˆ›å»ºä¸€ä¸ªä¸»æ—¥å¿—è®°å½•å™¨ï¼ˆé¿å…é‡å¤æ—¥å¿—ï¼‰
logger = logging.getLogger(__name__)
if not logger.handlers:  # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # æ§åˆ¶å°å¤„ç†å™¨
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)

    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    import os
    if not os.path.exists("logs"):
        os.makedirs("logs")

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¸¦è½®æ¢ï¼‰
    file_handler = RotatingFileHandler("logs/rss_qq_app.log", maxBytes=1024*1024, backupCount=5, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—å¤„ç†å™¨
    logger.handlers[0].flush()
    logger.handlers[1].flush()

# ä¸ºapschedulerå•ç‹¬è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('apscheduler').setLevel(logging.WARNING)

# å…¨å±€è°ƒåº¦å™¨å˜é‡ - ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤
scheduler = None
scheduler_lock = threading.Lock()
SENT_ENTRIES_FILE = "sent_entries.json"
sent_entries_lock = threading.Lock()

def load_system_state():
    """åŠ è½½ç³»ç»ŸçŠ¶æ€ï¼ˆåŒ…å«å·²å‘é€æ¡ç›®ã€é¦–æ¬¡è¿è¡ŒçŠ¶æ€ã€æ—¶é—´åˆ†ç•Œç‚¹ï¼‰"""
    with sent_entries_lock:
        default_state = {
            "sent_entries": [],
            "first_run_completed": False,
            "last_processed_time": {}
        }
        
        if not os.path.exists(SENT_ENTRIES_FILE):
            return default_state
        
        try:
            with open(SENT_ENTRIES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœæ–‡ä»¶åªåŒ…å«æ•°ç»„ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
            if isinstance(data, list):
                return {
                    "sent_entries": data,
                    "first_run_completed": True,  # å¦‚æœæœ‰æ—§æ•°æ®ï¼Œè¯´æ˜ä¸æ˜¯é¦–æ¬¡è¿è¡Œ
                    "last_processed_time": {}
                }
            
            # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å­—æ®µå­˜åœ¨
            state = default_state.copy()
            state.update(data)
            return state
            
        except Exception as e:
            logging.error(f"åŠ è½½ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return default_state

def save_system_state(sent_entries, first_run_completed, last_processed_time):
    """ä¿å­˜ç³»ç»ŸçŠ¶æ€"""
    with sent_entries_lock:
        try:
            state = {
                "sent_entries": list(sent_entries),
                "first_run_completed": first_run_completed,
                "last_processed_time": last_processed_time
            }
            
            with open(SENT_ENTRIES_FILE, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")

def load_sent_entries():
    """åŠ è½½å·²å‘é€æ¡ç›®è®°å½•ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
    state = load_system_state()
    return set(state["sent_entries"])

def is_first_run():
    """æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ"""
    state = load_system_state()
    return not state["first_run_completed"]

def load_last_processed_time(rss_url):
    """åŠ è½½RSSæºçš„æœ€åå¤„ç†æ—¶é—´"""
    state = load_system_state()
    return state["last_processed_time"].get(rss_url)

def save_last_processed_time(rss_url, timestamp):
    """ä¿å­˜RSSæºçš„æœ€åå¤„ç†æ—¶é—´"""
    state = load_system_state()
    state["last_processed_time"][rss_url] = timestamp
    
    # ä¿å­˜æ•´ä¸ªçŠ¶æ€
    save_system_state(
        set(state["sent_entries"]),
        state["first_run_completed"],
        state["last_processed_time"]
    )

def mark_first_run_completed():
    """æ ‡è®°é¦–æ¬¡è¿è¡Œå®Œæˆ"""
    state = load_system_state()
    state["first_run_completed"] = True
    
    # ä¿å­˜æ•´ä¸ªçŠ¶æ€
    save_system_state(
        set(state["sent_entries"]),
        True,
        state["last_processed_time"]
    )

def process_and_send():
    """å¤„ç†RSSå¹¶å‘é€æ¶ˆæ¯çš„ä¸»å‡½æ•°"""
    # æ£€æŸ¥åœæ­¢æ ‡å¿—
    if _stop_flag.is_set():
        logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡RSSå¤„ç†")
        return
        
    configs = load_rss_configs()
    if not configs:
        logger.info("æ²¡æœ‰é…ç½®çš„RSSæºï¼Œè·³è¿‡å¤„ç†")
        return

    # åŠ è½½å®Œæ•´é…ç½®ï¼ˆAffiliateConverteréœ€è¦å®Œæ•´é…ç½®ï¼‰
    from src.core.config_manager import load_config
    full_config = load_config()
    affiliate_config = full_config.get('affiliate_config', {})
    
    affiliate_converter = None
    if affiliate_config and any(affiliate_config.get(platform, {}).get('enabled', False)
                              for platform in ['dataoke', 'jingpinku', 'pdd']):
        affiliate_converter = AffiliateConverter(full_config)  # ä¼ é€’å®Œæ•´é…ç½®
        logger.info("è¿”åˆ©è½¬é“¾åŠŸèƒ½å·²å¯ç”¨")
    else:
        logger.info("è¿”åˆ©è½¬é“¾åŠŸèƒ½æœªé…ç½®ï¼Œå°†ä½¿ç”¨åŸå§‹é“¾æ¥")

    sent_entries = load_sent_entries()
    first_run = is_first_run()
    
    if first_run:
        logger.info("é¦–æ¬¡å¯åŠ¨ï¼Œå¯ç”¨ä¿æŠ¤æœºåˆ¶ - åªå¤„ç†æœ€æ–°10æ¡")
    
    for config in configs:
        try:
            entries = parse_feed(config["rss_url"])
            if not entries:
                logger.warning(f"RSSæº '{config['rss_url']}' æœªè¿”å›ä»»ä½•å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
                continue

            # è·å–æœ€åå¤„ç†æ—¶é—´
            last_processed_time = load_last_processed_time(config["rss_url"])
            
            if first_run:
                # é¦–æ¬¡å¯åŠ¨ï¼šåªå¤„ç†æœ€æ–°10æ¡
                entries = entries[:10]
                logger.info(f"é¦–æ¬¡å¯åŠ¨ä¿æŠ¤ï¼šRSSæº '{config['rss_url']}' åªå¤„ç†æœ€æ–° {len(entries)} æ¡")
                
                # è®°å½•é¦–æ¬¡å¯åŠ¨å¤„ç†çš„ç¬¬ä¸€æ¡çº¿æŠ¥æ—¶é—´ä½œä¸ºåˆ†ç•Œç‚¹
                if entries:
                    # RSSæ¡ç›®é€šå¸¸æŒ‰æ—¶é—´é™åºæ’åˆ—ï¼Œå–ç¬¬ä¸€æ¡ï¼ˆæœ€æ–°çš„ï¼‰ä½œä¸ºæ—¶é—´åˆ†ç•Œç‚¹
                    # åç»­è¿è¡Œåªå¤„ç†æ—¶é—´æ™šäºè¿™ä¸ªåˆ†ç•Œç‚¹çš„æ–°çº¿æŠ¥
                    first_entry = entries[0]
                    if hasattr(first_entry, 'published_parsed') and first_entry.published_parsed:
                        import time
                        import calendar
                        # æ­£ç¡®å¤„ç†æ—¶åŒºï¼šRSSæ—¶é—´é€šå¸¸æ˜¯UTCï¼Œéœ€è¦è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´æˆ³
                        cutoff_time = calendar.timegm(first_entry.published_parsed)
                        save_last_processed_time(config["rss_url"], cutoff_time)
                        # æ˜¾ç¤ºåŒ—äº¬æ—¶é—´
                        beijing_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cutoff_time))
                        logger.info(f"è®¾ç½®æ—¶é—´åˆ†ç•Œç‚¹ï¼š{beijing_time} (åŒ—äº¬æ—¶é—´)")
            else:
                # éé¦–æ¬¡å¯åŠ¨ï¼šåªå¤„ç†æ—¶é—´æ™šäºåˆ†ç•Œç‚¹çš„æ¡ç›®
                if last_processed_time:
                    import time as time_module
                    import calendar
                    filtered_entries = []
                    for entry in entries:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            # ç»Ÿä¸€ä½¿ç”¨calendar.timegmå¤„ç†RSSæ—¶é—´ï¼ˆUTCè½¬æ—¶é—´æˆ³ï¼‰
                            entry_time = calendar.timegm(entry.published_parsed)
                            if entry_time > last_processed_time:
                                filtered_entries.append(entry)
                    
                    entries = filtered_entries
                    if entries:
                        logger.info(f"æ—¶é—´è¿‡æ»¤ï¼šRSSæº '{config['rss_url']}' å‘ç° {len(entries)} æ¡æ–°çº¿æŠ¥")
                    else:
                        logger.info(f"æ—¶é—´è¿‡æ»¤ï¼šRSSæº '{config['rss_url']}' æ²¡æœ‰æ–°çº¿æŠ¥")
                        continue

            # æ‰¹é‡è½¬é“¾æ§åˆ¶
            batch_settings = affiliate_config.get('batch_settings', {}) if affiliate_config else {}
            max_convert_per_batch = batch_settings.get('max_convert_per_batch', 5)
            convert_enabled = batch_settings.get('convert_enabled', True)
            
            processed_count = 0
            converted_count = 0  # å½“å‰æ‰¹æ¬¡å·²è½¬é“¾æ•°é‡
            
            for entry in entries:
                # åœ¨å¾ªç¯ä¸­æ£€æŸ¥åœæ­¢æ ‡å¿—
                if _stop_flag.is_set():
                    logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ–­RSSæ¡ç›®å¤„ç†")
                    return
                    
                try:
                    entry_id = generate_entry_id(config["rss_url"], entry)
                    
                    # å»é‡æœºåˆ¶ï¼šè·³è¿‡å·²å‘é€çš„æ¡ç›®
                    if entry_id in sent_entries:
                        continue

                    clean_title = clean_html_tags(entry.title) if entry.title else "æ— æ ‡é¢˜"
                    link = getattr(entry, 'link', '') or ''
                    
                    # è·å–ç½‘é¡µå†…å®¹
                    webpage_content = ""
                    if link:
                        webpage_content = fetch_webpage_content(link)
                    
                    # å†…å®¹å¤„ç†ç­–ç•¥ï¼šä½¿ç”¨é«˜çº§æ¸…ç†åŠŸèƒ½
                    message_content = ""
                    raw_content = ""
                    
                    if webpage_content:
                        raw_content = webpage_content
                    else:
                        # å¦‚æœæ²¡æœ‰ç½‘é¡µå†…å®¹ï¼Œä½¿ç”¨RSSæ‘˜è¦
                        raw_content = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                    
                    if raw_content:
                        # ä½¿ç”¨é«˜çº§æ–‡æœ¬æ¸…ç†åŠŸèƒ½
                        from src.utils.text_cleaner import advanced_text_cleanup
                        message_content = advanced_text_cleanup(raw_content, clean_title, max_length=1200)
                    
                    # å¦‚æœæ¸…ç†åå†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨æ ‡é¢˜
                    if not message_content:
                        message_content = clean_title

                    # æ‰¹é‡è¿”åˆ©è½¬é“¾å¤„ç†ï¼šæ§åˆ¶æ¯æ‰¹è½¬é“¾æ•°é‡
                    should_convert = (convert_enabled and
                                    affiliate_converter and
                                    message_content and
                                    converted_count < max_convert_per_batch)
                    
                    if should_convert:
                        try:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯è½¬æ¢çš„å•†å“é“¾æ¥
                            urls = affiliate_converter._extract_urls(message_content)
                            if urls:  # åªæœ‰åŒ…å«å•†å“é“¾æ¥æ‰è¿›è¡Œè½¬æ¢
                                converted_content = affiliate_converter.convert_links(message_content)
                                if converted_content != message_content:
                                    logger.info(f"æˆåŠŸè½¬æ¢è¿”åˆ©é“¾æ¥ ({converted_count + 1}/{max_convert_per_batch}): {entry_id}")
                                    message_content = converted_content
                                    converted_count += 1
                        except Exception as e:
                            logger.error(f"è¿”åˆ©è½¬é“¾å¤„ç†å¤±è´¥: {e}")
                            # è½¬é“¾å¤±è´¥æ—¶ä¿ç•™åŸå†…å®¹ï¼Œä¸å½±å“æ¨é€

                    # é•¿åº¦æ§åˆ¶ï¼šè‡ªåŠ¨æˆªæ–­è¿‡é•¿å†…å®¹
                    if len(message_content) > 1500:
                        message_content = message_content[:1500] + "..."
                        logger.info(f"å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­: {entry_id}")

                    # æ¨é€ç­–ç•¥ï¼šåœ¨æ¶ˆæ¯æœ«å°¾æ·»åŠ åŸé“¾æ¥
                    if link:
                        if message_content:
                            message_content += f"\n\nğŸ“° å®Œæ•´çº¿æŠ¥ï¼š{link}"
                        else:
                            message_content = f"ğŸ“° å®Œæ•´çº¿æŠ¥ï¼š{link}"

                    # å‘é€æ¶ˆæ¯
                    if message_content and send_group_message(config['llonebot_api_url'], config["group_id"], message_content):
                        sent_entries.add(entry_id)
                        processed_count += 1
                        logger.info(f"æˆåŠŸæ¨é€: {clean_title[:50]}...")
                    else:
                        logger.warning(f"æ¨é€å¤±è´¥: {clean_title[:50]}...")
                
                except Exception as e:
                    logger.error(f"å¤„ç†RSSæ¡ç›® '{getattr(entry, 'title', 'N/A')}' æ—¶å‡ºé”™: {e}", exc_info=True)
            
            # æ›´æ–°æ—¶é—´åˆ†ç•Œç‚¹ï¼šå¤„ç†å®Œæˆåï¼Œå°†æœ€æ–°æ¡ç›®çš„æ—¶é—´è®¾ä¸ºæ–°çš„åˆ†ç•Œç‚¹
            if processed_count > 0:
                logger.info(f"RSSæº '{config['rss_url']}' æˆåŠŸå¤„ç† {processed_count} æ¡æ–°å†…å®¹")
                
                # æ›´æ–°æ—¶é—´åˆ†ç•Œç‚¹åˆ°æœ€æ–°å¤„ç†çš„æ¡ç›®
                if entries:
                    # å–ç¬¬ä¸€æ¡æ¡ç›®ï¼ˆæœ€æ–°çš„ï¼‰ä½œä¸ºæ–°çš„æ—¶é—´åˆ†ç•Œç‚¹
                    latest_entry = entries[0]
                    if hasattr(latest_entry, 'published_parsed') and latest_entry.published_parsed:
                        import time
                        import calendar
                        # æ­£ç¡®å¤„ç†æ—¶åŒºï¼šRSSæ—¶é—´é€šå¸¸æ˜¯UTCï¼Œéœ€è¦è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´æˆ³
                        new_cutoff_time = calendar.timegm(latest_entry.published_parsed)
                        save_last_processed_time(config["rss_url"], new_cutoff_time)
                        # æ˜¾ç¤ºåŒ—äº¬æ—¶é—´
                        beijing_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(new_cutoff_time))
                        logger.info(f"æ›´æ–°æ—¶é—´åˆ†ç•Œç‚¹ï¼š{beijing_time} (åŒ—äº¬æ—¶é—´)")
        
        except Exception as e:
            logger.error(f"å¤„ç†RSSæº '{config.get('rss_url', 'N/A')}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    
    # ä¿å­˜ç³»ç»ŸçŠ¶æ€ï¼ˆåŒ…æ‹¬å·²å‘é€è®°å½•ã€é¦–æ¬¡è¿è¡Œæ ‡è®°ç­‰ï¼‰
    state = load_system_state()
    save_system_state(
        sent_entries,
        True if first_run else state["first_run_completed"],
        state["last_processed_time"]
    )
    
    # æ ‡è®°é¦–æ¬¡è¿è¡Œå®Œæˆ
    if first_run:
        logger.info("é¦–æ¬¡å¯åŠ¨ä¿æŠ¤æœºåˆ¶å·²å®Œæˆ")

def update_scheduler(scheduler_instance, configs):
    """æ›´æ–°è°ƒåº¦å™¨"""
    if scheduler_instance:
        scheduler_instance.remove_all_jobs()
        for config in configs:
            interval = config.get("interval", 60)
            job_id = f"rss_{hash(config['rss_url'])}"
            scheduler_instance.add_job(
                process_and_send,
                'interval',
                minutes=interval,
                id=job_id,
                replace_existing=True
            )
            logger.info(f"å·²æ·»åŠ å®šæ—¶ä»»åŠ¡: {config['rss_url']} (é—´éš”: {interval}åˆ†é’Ÿ)")

def start_scheduler():
    """å¯åŠ¨è°ƒåº¦å™¨"""
    global scheduler
    
    with scheduler_lock:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è°ƒåº¦å™¨åœ¨è¿è¡Œ
        if scheduler and scheduler.running:
            logger.info("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­ï¼Œè·³è¿‡å¯åŠ¨")
            return
            
        # æ¸…é™¤åœæ­¢æ ‡å¿—
        _stop_flag.clear()
        
        configs = load_rss_configs()
        if configs:
            # åˆ›å»ºæ–°çš„è°ƒåº¦å™¨å®ä¾‹
            scheduler = BackgroundScheduler()
            update_scheduler(scheduler, configs)
            scheduler.start()
            logger.info("RSSç›‘æ§è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œå°†æŒ‰é…ç½®çš„é—´éš”æ—¶é—´æ‰§è¡Œå®šæ—¶æ¨é€")
        else:
            logger.info("æ²¡æœ‰é…ç½®RSSæºï¼Œè°ƒåº¦å™¨æœªå¯åŠ¨")

def stop_scheduler():
    """åœæ­¢è°ƒåº¦å™¨"""
    global scheduler
    
    # è®¾ç½®åœæ­¢æ ‡å¿—
    _stop_flag.set()
    logger.info("å·²è®¾ç½®åœæ­¢æ ‡å¿—")
    
    with scheduler_lock:
        if scheduler and scheduler.running:
            try:
                # å¼ºåˆ¶åœæ­¢æ‰€æœ‰ä»»åŠ¡
                scheduler.remove_all_jobs()
                logger.info("å·²ç§»é™¤æ‰€æœ‰è°ƒåº¦ä»»åŠ¡")
                
                # ç­‰å¾…è°ƒåº¦å™¨å®Œå…¨åœæ­¢
                scheduler.shutdown(wait=True)
                logger.info("è°ƒåº¦å™¨å·²å®Œå…¨åœæ­¢")
                
                scheduler = None
                logger.info("RSSç›‘æ§è°ƒåº¦å™¨å·²åœæ­¢å¹¶æ¸…ç©º")
            except Exception as e:
                logger.error(f"åœæ­¢è°ƒåº¦å™¨æ—¶å‡ºé”™: {e}")
                # å¼ºåˆ¶è®¾ç½®ä¸ºNone
                scheduler = None
        else:
            logger.info("è°ƒåº¦å™¨æœªåœ¨è¿è¡Œ")

def main():
    """ä¸»å‡½æ•° - å¯åŠ¨GUIåº”ç”¨"""
    app = QApplication(sys.argv)
    
    # è®¾ç½®åº”ç”¨ä¿¡æ¯
    app.setApplicationName("æ™ºèƒ½RSSçº¿æŠ¥æ¨é€ç³»ç»Ÿ")
    app.setApplicationVersion("4.0")
    app.setOrganizationName("RSSæ¨é€åŠ©æ‰‹")
    
    # åˆ›å»ºä¸»çª—å£
    main_win = MainWindow()
    main_win.show()
    
    # å¯åŠ¨åº”ç”¨å¾ªç¯
    sys.exit(app.exec())

if __name__ == "__main__":
    main()